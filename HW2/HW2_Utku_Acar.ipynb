{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS555.V: Data Science and Business Strategy\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Author:** Utku Acar  \n",
    "**Department:** Computer Science  \n",
    "**Role:** Software Engineer / Researcher  \n",
    "\n",
    "**My Secret Power:** I possess a remarkable ability to efficiently harness prompts and tame the wildest multi-dimensional data, particularly in the captivating realms of Images and Videos. My expertise lies in the magical world of Deep Learning and Computer Vision.\n",
    "\n",
    "**My Crime-Fighting Identity:** As the illustrious Hyperion Solitude, I embark on an exhilarating journey where the realms of data science merge with supercharged strategies. üöÄüîçüìäü¶∏‚Äç‚ôÇÔ∏è  \n",
    "\n",
    "Welcome to the enthralling odyssey that lies ahead! Prepare to witness the fusion of data science and business strategy in ways that push the boundaries of exploration and innovation. Join me in this adventure of a lifetime as I unveil insights, strategies, and solutions that spark positive change.\n",
    "\n",
    "Stay tuned for a remarkable journey that's about to unfold at [https://github.com/hyperionsolitude](https://github.com/hyperionsolitude). Together, we'll explore uncharted territories, conquer challenges, and celebrate the triumphs of data-driven exploration. üåüüéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accurate Housing Price Prediction with Data Cleaning and Manipulation Detection\n",
    "\n",
    "In this notebook, we're about to embark on a fascinating journey into the realm of accurate housing price prediction. Our mission revolves around overcoming the challenges posed by manipulated housing data and varying pricing practices. Through a blend of sophisticated data cleaning techniques and insightful manipulation detection, we'll develop a powerful predictive model that can provide reliable housing price estimates.\n",
    "\n",
    "## The Data\n",
    "\n",
    "Our dataset, named \"KonutGame.csv,\" unveils the intricate world of housing prices based on title deeds. However, the path to accuracy is fraught with obstacles. Taxes are determined by declared prices, and discrepancies can arise due to strategic pricing tactics and the influence of housing loans. Moreover, we're well aware that some individuals might sidestep the 90% housing loan regulation, further complicating the landscape.\n",
    "\n",
    "Our objective is to engineer a methodology that accurately calculates housing prices, taking into account potential manipulations and outliers, all while adhering to the regulatory guidelines.\n",
    "\n",
    "## Cleaning and Manipulation Detection\n",
    "\n",
    "Our approach to this challenge will encompass the following key phases:\n",
    "\n",
    "1. **Data Loading and Initial Exploration:** We will start by loading the \"KonutGame.csv\" dataset and delving into its structure and contents. An initial exploration will provide insights into the features that shape housing prices.\n",
    "\n",
    "2. **Data Cleaning and Preprocessing:** We'll embark on a comprehensive data cleaning journey. This includes handling missing values, addressing outliers, and ensuring data consistency to build a robust foundation for accurate predictions.\n",
    "\n",
    "3. **Manipulation Detection:** Armed with domain knowledge and analytical tools, we will unveil potential manipulations and irregularities in the housing data. By detecting instances of non-compliance with regulations and loan-pricing tactics, we'll lay the groundwork for more precise predictions.\n",
    "\n",
    "4. **Predictive Model Development:** With cleaned and validated data in hand, we'll design a predictive model that captures the nuances of housing price dynamics. Our model will account for the factors affecting prices while accommodating variations and regulatory influences.\n",
    "\n",
    "5. **Performance Evaluation and Insights:** We'll rigorously evaluate the performance of our predictive model using appropriate metrics. Through insights gained from the model's predictions, we'll gain a deeper understanding of the factors that most significantly impact housing prices.\n",
    "\n",
    "6. **Regulation-Aware Pricing:** Leveraging the power of our model, we'll embark on a journey to estimate accurate housing prices that adhere to regulations and detect instances where regulations are potentially circumvented.\n",
    "\n",
    "Our expedition promises a thrilling fusion of data science, domain expertise, and regulatory insights. As we explore the uncharted territories of housing price prediction, we invite you to join us in this captivating adventure of unraveling manipulation, enhancing accuracy, and decoding the intricate world of housing economics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "## Loading and Handling Missing Values\n",
    "\n",
    "In this initial step, we begin by loading the dataset from the \"KonutGame.csv\" file using the Pandas library. The loaded data contains information about housing prices based on title deeds. By using the `read_csv` function, we create a DataFrame called `data`.\n",
    "\n",
    "The first line of code, `data = pd.read_csv(\"KonutGame.csv\")`, loads the data into the DataFrame. The provided output `(17178, 9)` indicates that the dataset initially has 17,178 rows and 9 columns.\n",
    "\n",
    "We then proceed to handle any missing values in the dataset. Missing values can potentially impact the quality of our analysis and predictions. The `dropna` function is used with the `inplace=True` parameter to remove rows with missing values from the DataFrame. This step is essential for data integrity and accurate results.\n",
    "\n",
    "Following this, the output `(17178, 9)` confirms that after removing the rows with missing values, the dataset remains the same size.\n",
    "\n",
    "\n",
    "By performing these preprocessing steps, we set the foundation for further analysis, modeling, and insights from the housing price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17178, 9)\n",
      "(17178, 9)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"KonutGame.csv\")\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "In this code snippet, we focus on cleaning and preprocessing a housing dataset using the Pandas library. The dataset, initially stored in a DataFrame named `df_org`, contains information about various housing attributes. We create another DataFrame named `df` to work with the cleaned data.\n",
    "\n",
    "## Cleaning \"building_age\" and \"total_floor_count\"\n",
    "\n",
    "We begin by defining functions to clean the \"building_age\" and \"total_floor_count\" columns. The `clean_age` function processes values in the \"building_age\" column, handling cases where values are expressed as ranges or contain \"ve √ºzeri\" (and above). This function ensures that numerical values are extracted and assigned appropriately to the \"building_age\" column in the DataFrame.\n",
    "\n",
    "The `clean_floor_count` function serves a similar purpose for the \"total_floor_count\" column. It processes range-based and \"ve √ºzeri\" values, computing a single value that represents the floor count effectively.\n",
    "\n",
    "## Cleaning \"floor_no\"\n",
    "\n",
    "The code then proceeds to clean the \"floor_no\" column, which represents the floor number of each housing unit. The `clean_floor_no` function takes into account both the \"floor_no\" and \"total_floor_count\" columns to ensure accurate cleaning. It handles various cases, including garden and basement floors, special cases like \"M√ºstakil,\" and computes appropriate floor numbers based on the provided information.\n",
    "\n",
    "## Applying Cleaning Functions\n",
    "\n",
    "The cleaning functions are applied to the respective columns using the `apply` method. For \"building_age\" and \"total_floor_count,\" the functions are directly applied to the columns using the `apply` method. For \"floor_no,\" the `clean_floor_no` function is applied using a lambda function along with the `axis=1` parameter to work row-wise.\n",
    "\n",
    "As a result of this cleaning process, the DataFrame `df` is transformed to contain cleaned and standardized values for \"building_age,\" \"total_floor_count,\" and \"floor_no.\" These preprocessing steps lay the foundation for subsequent analysis and modeling tasks on the housing dataset.\n",
    "\n",
    "Stay tuned as we continue to explore and refine the data to extract meaningful insights and build predictive models for housing price estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_org = pd.DataFrame(data)\n",
    "\n",
    "# Clean \"building_age\" and \"total_floor_count\"\n",
    "def clean_age(age):\n",
    "    if isinstance(age, str):\n",
    "        if \"arasƒ±\" in age:\n",
    "            match = re.match(r'(\\d+)-(\\d+) arasƒ±', age)\n",
    "            if match:\n",
    "                lower_limit = int(match.group(1))\n",
    "                upper_limit = int(match.group(2))\n",
    "                return (lower_limit + upper_limit) // 2\n",
    "        elif \"ve √ºzeri\" in age:\n",
    "            return int(age.split(\" \")[0])  # Extract the lower limit\n",
    "    return int(age)\n",
    "\n",
    "df[\"building_age\"] = df[\"building_age\"].apply(clean_age)\n",
    "\n",
    "# Clean \"total_floor_count\"\n",
    "def clean_floor_count(floor_count):\n",
    "    if isinstance(floor_count, int):\n",
    "        return floor_count\n",
    "    elif \"arasƒ±\" in floor_count:\n",
    "        match = re.match(r'(\\d+)-(\\d+) arasƒ±', floor_count)\n",
    "        if match:\n",
    "            lower_limit = int(match.group(1))\n",
    "            upper_limit = int(match.group(2))\n",
    "            return (lower_limit + upper_limit) // 2\n",
    "    elif \"ve √ºzeri\" in floor_count:\n",
    "        return int(re.findall(r'\\d+', floor_count)[0])  # Extract the lower limit\n",
    "    elif \"Bah√ße katƒ±\" in floor_count:\n",
    "        return 1\n",
    "    else:\n",
    "        return int(floor_count)\n",
    "\n",
    "df[\"total_floor_count\"] = df[\"total_floor_count\"].apply(clean_floor_count)\n",
    "\n",
    "# Clean \"floor_no\"\n",
    "def clean_floor_no(floor_no, total_floor_count):\n",
    "    if \"Bah√ße\" in floor_no or \"Giri≈ü\" in floor_no or \"Zemin\" in floor_no:\n",
    "        return 1\n",
    "    elif \"Bodrum\" in floor_no:\n",
    "        return 0\n",
    "    elif \"M√ºstakil\" in floor_no or \"√úst\" in floor_no or \"Teras\" in floor_no or \"√áatƒ±\" in floor_no:\n",
    "        return total_floor_count\n",
    "    elif \"Kot\" in floor_no:\n",
    "        return int(floor_no.split(\"Kot\")[1])\n",
    "    elif \"ve √ºzeri\" in floor_no:\n",
    "        return int(floor_no.split(\" \")[0])  # Extract the lower limit\n",
    "    else:\n",
    "        return int(floor_no.split(\"+\")[0])\n",
    "\n",
    "df[\"floor_no\"] = df.apply(lambda row: clean_floor_no(row[\"floor_no\"], row[\"total_floor_count\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Encoding of Categorical Variables\n",
    "\n",
    "In this code segment, we delve into numerical encoding of categorical variables within the housing dataset using the Pandas library.\n",
    "\n",
    "## Numerize \"District\" and \"Neighborhood\"\n",
    "\n",
    "We take strides toward transforming categorical columns, \"District\" and \"Neighborhood,\" into numerical representations. The technique employed here is called numerical encoding, which allows us to convert text-based categories into corresponding numerical codes. This process facilitates the inclusion of categorical data in machine learning models that require numerical input.\n",
    "\n",
    "The code `df[\"District\"] = pd.factorize(df[\"District\"])[0]` showcases the application of the `factorize` function on the \"District\" column. This function assigns unique numerical codes to each distinct category in the column, transforming it into a numerical format. The same transformation is executed for the \"Neighborhood\" column using the code `df[\"Neighborhood\"] = pd.factorize(df[\"Neighborhood\"])[0]`.\n",
    "\n",
    "## Exporting Cleaned DataFrame\n",
    "\n",
    "We proceed by exporting the resulting DataFrame, which now incorporates numerical encodings for the \"District\" and \"Neighborhood\" columns. This refined DataFrame can serve as input for subsequent analysis and modeling tasks.\n",
    "\n",
    "The code snippet `df.to_csv(\"cleaned_data.csv\", index=False)` accomplishes the export of the cleaned DataFrame to a CSV file named \"cleaned_data.csv.\" The `index=False` parameter ensures that the DataFrame indices are not included in the exported file.\n",
    "\n",
    "With categorical variables numerically encoded and the data exported in a refined format, we are well-equipped to undertake sophisticated analyses and predictive modeling based on the enriched dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerize \"District\" and \"Neighborhood\"\n",
    "df[\"District\"] = pd.factorize(df[\"District\"])[0]\n",
    "df[\"Neighboorhood\"] = pd.factorize(df[\"Neighboorhood\"])[0]\n",
    "\n",
    "# Export the resulting DataFrame to a CSV file\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of 90% Regulation Feature\n",
    "\n",
    "In this section of code, we delve into the implementation of a crucial feature based on the 90% regulation principle. The objective is to determine whether a particular housing unit adheres to the regulation that housing loans cannot exceed 90% of the property's value. We leverage the NumPy library for efficient calculations.\n",
    "\n",
    "## Adding the \"ninetyreg\" Feature\n",
    "\n",
    "The code `df['ninetyreg'] = np.where(df['mortgage'] >= 0.9 * df['value'], 1, 0)` introduces a new feature named \"ninetyreg\" into the DataFrame. This feature serves as a binary indicator, where a value of 1 indicates compliance with the 90% regulation and 0 signifies non-compliance. The calculation is achieved by comparing the mortgage value to 90% of the property's value, employing the NumPy function `np.where`.\n",
    "\n",
    "## Displaying the Updated DataFrame\n",
    "\n",
    "We conclude this section by presenting an overview of the updated DataFrame. The code `df['ninetyreg']` offers a glimpse into the newly added \"ninetyreg\" feature. Each entry in this column signifies whether the housing unit adheres to the 90% regulation or not.\n",
    "\n",
    "As we proceed through the data exploration and analysis journey, this \"ninetyreg\" feature will serve as a critical piece of information, enabling us to better understand the compliance landscape and its potential influence on housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "17173    0\n",
       "17174    0\n",
       "17175    0\n",
       "17176    0\n",
       "17177    0\n",
       "Name: ninetyreg, Length: 17178, dtype: int64"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add the \"ninetyreg\" feature based on the 90% regulation\n",
    "df['ninetyreg'] = np.where(df['mortgage'] >= 0.9 * df['value'], 1, 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df['ninetyreg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Regulated Prices Feature\n",
    "\n",
    "In this section, we delve into the creation of a new feature that captures regulated (true) prices of housing units, taking into account the 90% regulation principle. This process involves utilizing the \"ninetyreg\" feature to determine the appropriate price based on compliance status.\n",
    "\n",
    "## Generating the \"regulated_value\" Feature\n",
    "\n",
    "The code `df['regulated_value'] = df.apply(lambda row: row['mortgage'] / 0.9 if row['ninetyreg'] == 1 else row['value'], axis=1)` introduces the \"regulated_value\" feature to the DataFrame. This feature encapsulates the regulated prices of housing units. For units adhering to the 90% regulation (as indicated by the \"ninetyreg\" feature), the regulated price is calculated by dividing the mortgage value by 0.9. In cases where the unit does not adhere to the regulation, the original \"value\" is retained as the regulated price.\n",
    "\n",
    "## Displaying Rows with Price Differences\n",
    "\n",
    "To gain insights into the impact of regulation on housing prices, we proceed to identify and display rows where the \"value\" and \"regulated_value\" differ. The code `price_difference_rows = df[df['value'] != df['regulated_value']]` filters the DataFrame to isolate rows where there's a discrepancy in values.\n",
    "\n",
    "We then extract and present the columns \"value\" and \"regulated_value\" for these specific rows. This visualization provides a clear view of the instances where regulatory considerations lead to adjusted housing prices.\n",
    "\n",
    "As we continue our exploration, the \"regulated_value\" feature will enable us to assess the regulatory impact on housing prices and deepen our understanding of the interplay between regulation, perceived value, and actual pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>regulated_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280000</td>\n",
       "      <td>311111.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>605000</td>\n",
       "      <td>672222.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>190000</td>\n",
       "      <td>211111.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>520000</td>\n",
       "      <td>577777.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>315000</td>\n",
       "      <td>350000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17109</th>\n",
       "      <td>235000</td>\n",
       "      <td>261111.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17138</th>\n",
       "      <td>400000</td>\n",
       "      <td>416666.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17139</th>\n",
       "      <td>325000</td>\n",
       "      <td>361111.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17142</th>\n",
       "      <td>275000</td>\n",
       "      <td>305555.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>180000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2495 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        value  regulated_value\n",
       "3      280000    311111.111111\n",
       "6      605000    672222.222222\n",
       "14     190000    211111.111111\n",
       "15     520000    577777.777778\n",
       "24     315000    350000.000000\n",
       "...       ...              ...\n",
       "17109  235000    261111.111111\n",
       "17138  400000    416666.666667\n",
       "17139  325000    361111.111111\n",
       "17142  275000    305555.555556\n",
       "17165  180000    200000.000000\n",
       "\n",
       "[2495 rows x 2 columns]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new feature to store the regulated(true) prices\n",
    "df['regulated_value'] = df.apply(lambda row: row['mortgage'] / 0.9 if row['ninetyreg'] == 1 else row['value'], axis=1)\n",
    "\n",
    "# Display rows where 'value' and 'regulated_value' are different\n",
    "price_difference_rows = df[df['value'] != df['regulated_value']]\n",
    "price_difference_rows[['value', 'regulated_value']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Key Metrics: Room Count and Value per Room\n",
    "\n",
    "In this section, we delve into the derivation of critical metrics related to housing attributes, specifically focusing on room count and value per room. These calculations enable us to gain insights into the housing layout and pricing structure.\n",
    "\n",
    "## Extracting Room Count from Room Count Descriptor\n",
    "\n",
    "We commence by defining a function, `extract_room_count(room_count)`, that facilitates the extraction of the number of rooms from the room count descriptor. The code `return sum(int(x) for x in room_count.split('+'))` processes the descriptor to tally the individual room numbers, generating a numerical count.\n",
    "\n",
    "We proceed to apply this function to the 'room_count' column using the code `df['room_count_num'] = df['room_count'].apply(extract_room_count)`. This application yields a new 'room_count_num' column, housing the derived room counts.\n",
    "\n",
    "## Calculating Average Room Size\n",
    "\n",
    "The code `df['avg_room_size'] = df['size'] / df['room_count_num']` embarks on the calculation of the average size per room, encapsulated in the 'avg_room_size' column. The division of the total size by the room count provides us with a metric that represents the spatial extent of individual rooms within a housing unit.\n",
    "\n",
    "## Determining Value per Room\n",
    "\n",
    "To gain insights into the housing value structure, we calculate the value per room, utilizing the \"regulated_value\" and \"avg_room_size\" columns. The code `df['value_per_room_by_m2'] = df['regulated_value'] / df['avg_room_size']` computes the value attributed to each room on a per-square-meter basis, revealing a nuanced perspective on pricing distribution.\n",
    "\n",
    "## Displaying the Enriched DataFrame\n",
    "\n",
    "We conclude by presenting an enriched DataFrame that encapsulates the newly calculated metrics. The code `df[['room_count', 'size', 'regulated_value', 'value_per_room_by_m2']]` offers a glimpse into the room count, total size, regulated value, and value per room metrics for each housing unit.\n",
    "\n",
    "These metrics open up avenues for understanding housing layouts, pricing dynamics, and their interplay within the context of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_count</th>\n",
       "      <th>size</th>\n",
       "      <th>regulated_value</th>\n",
       "      <th>value_per_room_by_m2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3+1</td>\n",
       "      <td>130</td>\n",
       "      <td>380000.000000</td>\n",
       "      <td>11692.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3+1</td>\n",
       "      <td>90</td>\n",
       "      <td>435000.000000</td>\n",
       "      <td>19333.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3+2</td>\n",
       "      <td>175</td>\n",
       "      <td>420000.000000</td>\n",
       "      <td>12000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2+1</td>\n",
       "      <td>80</td>\n",
       "      <td>311111.111111</td>\n",
       "      <td>11666.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2+1</td>\n",
       "      <td>88</td>\n",
       "      <td>345000.000000</td>\n",
       "      <td>11761.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17173</th>\n",
       "      <td>3+1</td>\n",
       "      <td>100</td>\n",
       "      <td>265000.000000</td>\n",
       "      <td>10600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17174</th>\n",
       "      <td>3+1</td>\n",
       "      <td>125</td>\n",
       "      <td>170000.000000</td>\n",
       "      <td>5440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>3+1</td>\n",
       "      <td>125</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>2+1</td>\n",
       "      <td>85</td>\n",
       "      <td>275000.000000</td>\n",
       "      <td>9705.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>2+1</td>\n",
       "      <td>85</td>\n",
       "      <td>115000.000000</td>\n",
       "      <td>4058.823529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17178 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      room_count  size  regulated_value  value_per_room_by_m2\n",
       "0            3+1   130    380000.000000          11692.307692\n",
       "1            3+1    90    435000.000000          19333.333333\n",
       "2            3+2   175    420000.000000          12000.000000\n",
       "3            2+1    80    311111.111111          11666.666667\n",
       "4            2+1    88    345000.000000          11761.363636\n",
       "...          ...   ...              ...                   ...\n",
       "17173        3+1   100    265000.000000          10600.000000\n",
       "17174        3+1   125    170000.000000           5440.000000\n",
       "17175        3+1   125    150000.000000           4800.000000\n",
       "17176        2+1    85    275000.000000           9705.882353\n",
       "17177        2+1    85    115000.000000           4058.823529\n",
       "\n",
       "[17178 rows x 4 columns]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to extract the number of rooms from the room count\n",
    "def extract_room_count(room_count):\n",
    "    return sum(int(x) for x in room_count.split('+'))\n",
    "\n",
    "# Apply the function to the 'room_count' column to create a new 'room_count_num' column\n",
    "df['room_count_num'] = df['room_count'].apply(extract_room_count)\n",
    "\n",
    "# Calculate the average size per room and create the 'avg_room_size' column\n",
    "df['avg_room_size'] = df['size'] / df['room_count_num']\n",
    "\n",
    "# Calculate value per room\n",
    "df['value_per_room_by_m2'] = df['regulated_value'] / df['avg_room_size']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['room_count', 'size','regulated_value','value_per_room_by_m2' ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District-Level Analysis: Value Per Room Variation\n",
    "\n",
    "In this section, we immerse ourselves in a district-level analysis, aiming to uncover the variation in value per room metrics across different districts. By dissecting this variation, we gain insights into the diverse pricing dynamics associated with varying localities.\n",
    "\n",
    "## Calculating Average Value Per Room for Each District\n",
    "\n",
    "We embark on our journey by calculating the average value per room within each district, leveraging the `groupby` functionality. The code `district_avg_value_per_room = df.groupby('District')['value_per_room_by_m2'].mean()` aggregates the value per room metrics for each district, furnishing us with a district-wise average.\n",
    "\n",
    "## Unveiling the Difference from District Average\n",
    "\n",
    "Our exploration takes an intriguing turn as we compute the difference between the value per room metric and the district average. The code `df['diff_from_district_avg'] = df.apply(lambda row: row['value_per_room_by_m2'] - district_avg_value_per_room[row['District']], axis=1)` equips each housing unit with a new metric, capturing the deviation from the district's average value per room. This deviation serves as a key indicator of how the unit's pricing aligns with or deviates from the district norm.\n",
    "\n",
    "## Displaying the Enriched DataFrame\n",
    "\n",
    "We culminate our analysis by presenting an enriched DataFrame, encompassing both the regulated value, value per room, and the insightful \"diff_from_district_avg\" metric. The code `df[['regulated_value', 'value_per_room_by_m2', 'diff_from_district_avg']]` unveils this composite view, offering a comprehensive snapshot of pricing, room valuation, and district-level variance.\n",
    "\n",
    "These metrics empower us to discern pricing anomalies, spot areas of high or low relative pricing, and unravel the unique pricing dynamics within each district. As our exploration deepens, these insights will guide us toward data-driven strategies and predictions that harness the essence of district-level variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regulated_value</th>\n",
       "      <th>value_per_room_by_m2</th>\n",
       "      <th>diff_from_district_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380000.000000</td>\n",
       "      <td>11692.307692</td>\n",
       "      <td>4905.344191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>435000.000000</td>\n",
       "      <td>19333.333333</td>\n",
       "      <td>-1205.104776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420000.000000</td>\n",
       "      <td>12000.000000</td>\n",
       "      <td>5213.036498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311111.111111</td>\n",
       "      <td>11666.666667</td>\n",
       "      <td>-3.982314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345000.000000</td>\n",
       "      <td>11761.363636</td>\n",
       "      <td>-5823.761033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17173</th>\n",
       "      <td>265000.000000</td>\n",
       "      <td>10600.000000</td>\n",
       "      <td>-9938.438109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17174</th>\n",
       "      <td>170000.000000</td>\n",
       "      <td>5440.000000</td>\n",
       "      <td>-6230.648980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>150000.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>-6870.648980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>275000.000000</td>\n",
       "      <td>9705.882353</td>\n",
       "      <td>-514.262271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>115000.000000</td>\n",
       "      <td>4058.823529</td>\n",
       "      <td>-3418.306805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17178 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       regulated_value  value_per_room_by_m2  diff_from_district_avg\n",
       "0        380000.000000          11692.307692             4905.344191\n",
       "1        435000.000000          19333.333333            -1205.104776\n",
       "2        420000.000000          12000.000000             5213.036498\n",
       "3        311111.111111          11666.666667               -3.982314\n",
       "4        345000.000000          11761.363636            -5823.761033\n",
       "...                ...                   ...                     ...\n",
       "17173    265000.000000          10600.000000            -9938.438109\n",
       "17174    170000.000000           5440.000000            -6230.648980\n",
       "17175    150000.000000           4800.000000            -6870.648980\n",
       "17176    275000.000000           9705.882353             -514.262271\n",
       "17177    115000.000000           4058.823529            -3418.306805\n",
       "\n",
       "[17178 rows x 3 columns]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average value_per_avg_room for each District\n",
    "district_avg_value_per_room = df.groupby('District')['value_per_room_by_m2'].mean()\n",
    "\n",
    "# Calculate the difference between value_per_avg_room and average value_per_avg_room for the District\n",
    "df['diff_from_district_avg'] = df.apply(lambda row: row['value_per_room_by_m2'] - district_avg_value_per_room[row['District']], axis=1)\n",
    "\n",
    "df[['regulated_value','value_per_room_by_m2', 'diff_from_district_avg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected Prices and Normalization: Unraveling the Housing Pricing Puzzle\n",
    "\n",
    "In this segment, we embark on a journey of recalibrating housing prices through the lens of corrected prices, normalization, and the strategic integration of randomness. Our exploration seeks to shed light on the methodology behind each step and the rationale driving our choices.\n",
    "\n",
    "## Step 2: Calculating Corrected Prices\n",
    "\n",
    "Our voyage begins with the calculation of corrected prices, where we address pricing nuances to establish a fair and consistent pricing framework. The function `correct_price(row)` takes center stage, harnessing the district's average value per room as a starting point. By strategically introducing controlled randomness, we account for fluctuations inherent in real-world pricing. The code `return district_avg + np.random.uniform(-0.1, 0.1)` encapsulates this process, yielding corrected prices that balance statistical insights with the unpredictability of the market.\n",
    "\n",
    "## Step 3: Normalization for Comprehensive Insight\n",
    "\n",
    "Normalization steps in as our guiding compass, enabling us to understand pricing on a holistic scale. We apply normalization to the corrected prices, factoring in the average room size. The code `df['normalized_price'] = df['corrected_price_per_room'] * df['avg_room_size']` generates normalized prices that account for variations in housing unit sizes, ensuring an even playing field for comparison.\n",
    "\n",
    "## Step 4: Visualizing the Data Transformation\n",
    "\n",
    "As we traverse through these transformations, we unveil a tableau of key metrics for visual examination. By extracting and presenting columns of interest, including \"value_per_room_by_m2,\" \"diff_from_district_avg,\" \"corrected_price_per_room,\" \"regulated_value,\" and \"normalized_price,\" we capture the essence of our journey in a succinct form. This visualization is a testament to the profound impact of data science on pricing strategies.\n",
    "\n",
    "## Strategic Decision-Making: Our Guiding Light\n",
    "\n",
    "Each decision in this process is a carefully considered step toward a balanced pricing paradigm. By incorporating district-level insights, calculated randomness, and normalization, we forge a path toward pricing models that echo real-world dynamics. The amalgamation of these techniques aligns with our commitment to infusing data science with strategic decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_per_room_by_m2</th>\n",
       "      <th>diff_from_district_avg</th>\n",
       "      <th>corrected_price_per_room</th>\n",
       "      <th>regulated_value</th>\n",
       "      <th>normalized_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11692.307692</td>\n",
       "      <td>4905.344191</td>\n",
       "      <td>6787.007153</td>\n",
       "      <td>380000.000000</td>\n",
       "      <td>220577.732474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19333.333333</td>\n",
       "      <td>-1205.104776</td>\n",
       "      <td>20538.505857</td>\n",
       "      <td>435000.000000</td>\n",
       "      <td>462116.381791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000.000000</td>\n",
       "      <td>5213.036498</td>\n",
       "      <td>6787.015433</td>\n",
       "      <td>420000.000000</td>\n",
       "      <td>237545.540169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11666.666667</td>\n",
       "      <td>-3.982314</td>\n",
       "      <td>11670.643387</td>\n",
       "      <td>311111.111111</td>\n",
       "      <td>311217.156984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11761.363636</td>\n",
       "      <td>-5823.761033</td>\n",
       "      <td>17585.133169</td>\n",
       "      <td>345000.000000</td>\n",
       "      <td>515830.572943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17173</th>\n",
       "      <td>10600.000000</td>\n",
       "      <td>-9938.438109</td>\n",
       "      <td>20538.438125</td>\n",
       "      <td>265000.000000</td>\n",
       "      <td>513460.953135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17174</th>\n",
       "      <td>5440.000000</td>\n",
       "      <td>-6230.648980</td>\n",
       "      <td>11670.741320</td>\n",
       "      <td>170000.000000</td>\n",
       "      <td>364710.666262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>4800.000000</td>\n",
       "      <td>-6870.648980</td>\n",
       "      <td>11670.559379</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>364704.980608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>9705.882353</td>\n",
       "      <td>-514.262271</td>\n",
       "      <td>10220.050500</td>\n",
       "      <td>275000.000000</td>\n",
       "      <td>289568.097493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>4058.823529</td>\n",
       "      <td>-3418.306805</td>\n",
       "      <td>7477.219168</td>\n",
       "      <td>115000.000000</td>\n",
       "      <td>211854.543089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17178 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value_per_room_by_m2  diff_from_district_avg  corrected_price_per_room  \\\n",
       "0              11692.307692             4905.344191               6787.007153   \n",
       "1              19333.333333            -1205.104776              20538.505857   \n",
       "2              12000.000000             5213.036498               6787.015433   \n",
       "3              11666.666667               -3.982314              11670.643387   \n",
       "4              11761.363636            -5823.761033              17585.133169   \n",
       "...                     ...                     ...                       ...   \n",
       "17173          10600.000000            -9938.438109              20538.438125   \n",
       "17174           5440.000000            -6230.648980              11670.741320   \n",
       "17175           4800.000000            -6870.648980              11670.559379   \n",
       "17176           9705.882353             -514.262271              10220.050500   \n",
       "17177           4058.823529            -3418.306805               7477.219168   \n",
       "\n",
       "       regulated_value  normalized_price  \n",
       "0        380000.000000     220577.732474  \n",
       "1        435000.000000     462116.381791  \n",
       "2        420000.000000     237545.540169  \n",
       "3        311111.111111     311217.156984  \n",
       "4        345000.000000     515830.572943  \n",
       "...                ...               ...  \n",
       "17173    265000.000000     513460.953135  \n",
       "17174    170000.000000     364710.666262  \n",
       "17175    150000.000000     364704.980608  \n",
       "17176    275000.000000     289568.097493  \n",
       "17177    115000.000000     211854.543089  \n",
       "\n",
       "[17178 rows x 5 columns]"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate corrected prices and add randomness\n",
    "def correct_price(row):\n",
    "    district_avg = district_avg_value_per_room[row['District']]\n",
    "    return district_avg + np.random.uniform(-0.1, 0.1) \n",
    "\n",
    "df['corrected_price_per_room'] = df.apply(correct_price, axis=1)\n",
    "\n",
    "# Calculate the normalized price\n",
    "df['normalized_price'] = df['corrected_price_per_room'] * df['avg_room_size']\n",
    "\n",
    "\n",
    "# Display the columns of interest\n",
    "df[['value_per_room_by_m2','diff_from_district_avg', 'corrected_price_per_room','regulated_value' ,'normalized_price']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Strategy for Dataset Split\n",
    "\n",
    "In the realm of predictive modeling, selecting the right set of features is akin to sculpting a masterpiece. The choices we make are a delicate interplay of context, goals, and data nuances. In this section, we delve into our thought process behind feature selection for both the cleared and uncleared datasets.\n",
    "\n",
    "## Feature Selection for Cleared Dataset (X_cleared)\n",
    "\n",
    "1. **`normalized_price`:** Serving as our target variable, this feature is wisely omitted from the feature set. After all, the aim is to predict it rather than include it as an input.\n",
    "\n",
    "2. **`room_count`:** The original room count feature may take a back seat here. Derived features like `avg_room_size` and `value_per_room_by_m2` seem to encapsulate the essence of room count's impact on pricing.\n",
    "\n",
    "3. **`value`:** Similar to `normalized_price`, this feature is the response variable we're striving to predict. Hence, it doesn't find a place in our feature set.\n",
    "\n",
    "4. **`regulated_value`:** This feature could potentially introduce data leakage due to its strong correlation with `normalized_price`. By excluding it, we maintain a cleaner predictive landscape.\n",
    "\n",
    "## Feature Selection for Uncleared Dataset (X_uncleared)\n",
    "\n",
    "1. **`value`:** In the uncleared dataset, this is our target variable for prediction. As a result, it gracefully steps aside from the feature ensemble.\n",
    "\n",
    "2. **`floor_no`:** This feature might be absent because it underwent categorical transformation in the cleared dataset. To ensure alignment, we exclude it here as well.\n",
    "\n",
    "3. **`room_count`:** Similar to the cleared dataset, the original room count feature yields the spotlight to its derived counterparts, promoting coherence in our approach.\n",
    "\n",
    "4. **`District` and `Neighboorhood`:** In the cleared dataset, these categorical variables metamorphosed into numerical features through one-hot encoding. Since this transformation isn't mirrored in the uncleared dataset, these features gracefully recede to maintain harmonious congruence.\n",
    "\n",
    "Our selection methodology for both datasets showcases a blend of consistency, precision, and prudent exclusion. The goal is to sculpt predictive models that marry the intricacies of data with strategic decisions. As we journey forward, these choices will serve as the bedrock upon which our modeling saga unfolds.\n",
    "\n",
    "Remember, every omission is a deliberate step toward unveiling the essence of predictive modeling‚Äîone that balances information with abstraction and clarity with complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Split the cleared dataset into features (X_cleared) and target (y_cleared)\n",
    "X_cleared = df.drop(['normalized_price','room_count','value','regulated_value'], axis=1)\n",
    "y_cleared = df['normalized_price']\n",
    "\n",
    "# Split the uncleared dataset into features (X_uncleared) and target (y_uncleared)\n",
    "X_uncleared = df_org.drop(['value','floor_no','room_count', 'District', 'Neighboorhood'], axis=1)\n",
    "y_uncleared = df_org['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split and Model Training: Illuminating the Cleared Path\n",
    "\n",
    "As our journey unfolds, we find ourselves at a critical juncture‚Äîwhere the alchemy of data partitioning and model training takes center stage. Let's illuminate this juncture with clarity as we delve into splitting our datasets and training a Linear Regression model on the cleared terrain.\n",
    "\n",
    "## Dividing the Landscape: Train-Test Split\n",
    "\n",
    "1. **Dataset Division:** Our cleared dataset embodies the promise of insights. To nurture these seeds of discovery, we split it into two harmonious realms‚Äîtraining and testing. In this choreography, the cleared data dons the roles of both mentor and examiner.\n",
    "\n",
    "2. **Deconstructed Symphony:** Behold the splendor of partitioning‚Äî`X_train_cleared` and `y_train_cleared` take up residence in the training realm, while `X_test_cleared` and `y_test_cleared` grace the testing realm. A thoughtful **80-20** split embraces equilibrium between learning and evaluation.\n",
    "\n",
    "## Enter the Linear Regression Virtuoso\n",
    "\n",
    "1. **Model's Prelude:** With data poised for exploration, our Linear Regression virtuoso graces the stage‚Äîthe **model_cleared**. In the cleared dataset's haven, it embarks on an expedition of insights, unraveling the narrative woven by our selected features.\n",
    "\n",
    "2. **Symphony of Learning:** The model dons the dual roles of student and conductor. As a student, it learns the essence of relationships between features and normalized prices, while as a conductor, it orchestrates the predictive symphony.\n",
    "\n",
    "## A Crescendo of Insight\n",
    "\n",
    "1. **Interplay of Features:** As our Linear Regression model learns, a crescendo of insight unfolds. Each feature's unique melody weaves into the overarching harmony of predictions.\n",
    "\n",
    "2. **Leveraging Cleared Wisdom:** The magic of feature selection is palpable, elevating the model's grasp of intrinsic patterns while sidestepping noise.\n",
    "\n",
    "## The Grand Finale Awaits\n",
    "\n",
    "Our journey through data partitioning and model training ignites the spark of anticipation. The **model_cleared**, fueled by the symphony of curated features, preludes the grand finale‚Äîthe moment of evaluation. Here, the promise of predictive mastery will be put to the test, and insights will dance on the stage of validation.\n",
    "\n",
    "As the curtain rises on the upcoming evaluation phase, the echo of cleared insights resonates. Join us in the anticipation of a harmonious revelation, where feature-driven clarity melds with the eloquence of model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the datasets into train and test sets\n",
    "X_train_cleared, X_test_cleared, y_train_cleared, y_test_cleared = train_test_split(X_cleared, y_cleared, test_size=0.2, random_state=42)\n",
    "X_train_uncleared, X_test_uncleared, y_train_uncleared, y_test_uncleared = train_test_split(X_uncleared, y_uncleared, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model on the cleared data\n",
    "model_cleared = LinearRegression()\n",
    "model_cleared.fit(X_train_cleared, y_train_cleared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illuminating the Prediction Stage: Unveiling the Crystal Ball\n",
    "\n",
    "The anticipation reaches its zenith as we approach the prediction stage‚Äîa moment of truth where our models take the center stage, wielding their predictive prowess. Let's shed light on this stage as we delve into the art of making predictions on the test set.\n",
    "\n",
    "## The Prediction Canvas\n",
    "\n",
    "1. **Cleared Model's Divination:** The spotlight shines on our cleared model‚Äîthe harbinger of insights forged through careful feature selection and meticulous data curation. With a flourish, the model unveils its predictions for the test set, encapsulating its understanding of cleared data dynamics.\n",
    "\n",
    "2. **Uncleared Model's Prophecy:** Parallel to the cleared model, our uncleared model makes its grand entrance. This model is unshackled by feature pruning, carrying the essence of the original dataset. It summons its predictive prowess to foretell outcomes for the test set.\n",
    "\n",
    "## The Clairvoyant Models\n",
    "\n",
    "1. **Model-Crafted Crystal Ball:** The cleared model, known as the **model_cleared**, gazes into the crystal ball of features it has been groomed upon. Drawing from its cultivated wisdom, it conjures predictions that mirror its comprehension of cleared data relationships.\n",
    "\n",
    "2. **The Enigmatic Oracle:** Behold the uncleared model‚Äî**model_uncleared**‚Äîa mystical oracle with a canvas of unfiltered features. It distills its predictions using the intricate tapestry of the original dataset, offering an alternate perspective on the future.\n",
    "\n",
    "## A Kaleidoscope of Insights\n",
    "\n",
    "1. **The Cleared Model's Focus:** Trained on the refined data landscape, the cleared model channels insights unique to its domain. Its predictions reflect a harmony between selected features and outcomes, shedding light on cleared data's predictive tapestry.\n",
    "\n",
    "2. **The Uncleared Model's Mirage:** The uncleared model, unburdened by feature constraints, casts a wider net. Its predictions capture the essence of unfiltered relationships within the dataset, potentially uncovering nuances unseen by its counterpart.\n",
    "\n",
    "## The Pendulum Swings\n",
    "\n",
    "Our journey through prediction takes us into the realms of speculation and foresight. As the cleared and uncleared models cast their predictions, we stand on the precipice of validation, where results will reveal the efficacy of our feature-driven strategies.\n",
    "\n",
    "With a flourish, we embrace the culmination‚Äîa moment where predictive mastery entwines with feature intricacies. Join us in the forthcoming revelation as predictions unravel the threads of model understanding and dataset complexity.\n",
    "\n",
    "Prepare for the grand unveiling‚Äîthe climax of the prediction journey that holds the key to understanding our models' predictive artistry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "pred_cleared = model_cleared.predict(X_test_cleared)\n",
    "pred_uncleared = model_uncleared.predict(X_test_uncleared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance: Decoding the Metrics Symphony\n",
    "\n",
    "As we stand at the crossroads of prediction and reality, we embark on a voyage of evaluating our models' predictive finesse. The curtain rises on a stage adorned with two metrics‚Äî**Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**. Let's unravel the significance of these metrics as they cast their spotlight on model performance.\n",
    "\n",
    "## The Performance Canvas\n",
    "\n",
    "1. **Cleared Model's Sonata:** With the cleared model in the limelight, the metrics take the stage. Our virtuoso, the **model_cleared**, showcases its MAE and RMSE scores as testament to its predictive prowess over curated data.\n",
    "\n",
    "2. **Uncleared Model's Ballad:** The uncleared model, **model_uncleared**, steps onto the stage, revealing its MAE and RMSE scores in a symphony of numbers. This enigmatic performer operates in a world unbound by feature constraints.\n",
    "\n",
    "## The Metrics Symphony\n",
    "\n",
    "1. **Mean Absolute Error (MAE):** The MAE, a reliable guide, measures the average magnitude of prediction errors. It gauges the distance between predicted and actual values, encapsulating the model's precision in its notes.\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE):** RMSE, a sonnet of the squared errors' magnitude, refines the narrative. It captures the average distance between predicted and actual values, offering a poignant reflection on model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance using different metrics\n",
    "mae_cleared = mean_absolute_error(y_test_cleared, pred_cleared)\n",
    "rmse_cleared = mean_squared_error(y_test_cleared, pred_cleared, squared=False)\n",
    "\n",
    "mae_uncleared = mean_absolute_error(y_test_uncleared, pred_uncleared)\n",
    "rmse_uncleared = mean_squared_error(y_test_uncleared, pred_uncleared, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scoreboard of Insight\n",
    "\n",
    "As the curtains rise on the realm of performance evaluation, a dynamic display of metrics takes center stage. Our models, like virtuoso conductors, translate raw data into harmonious predictions. Amidst this symphony of numbers, a profound difference emerges‚Äîone driven by the transformative power of cleared data.\n",
    "\n",
    "The cleared model, fortified by its refined features, proudly displays its symmetrical scores. With an astounding ratio of **1:10** in favor of cleared data, its predictive prowess resounds like a precise melodic composition. Every feature, every nuance, contributes to an orchestration of accuracy, proving that data curation leads to predictive mastery.\n",
    "\n",
    "Yet, the uncleared model's scores echo a different narrative‚Äîa tale of wandering through a diverse feature landscape without guidance. A ratio of **1:10** unveils the impact of ignoring curation‚Äîa stark reminder of how unchecked data diversity can lead to discordant predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared Data Performance:\n",
      "MAE: 12612.760773877653\n",
      "RMSE: 21259.401722036604\n",
      "\n",
      "Uncleared Data Performance:\n",
      "MAE: 116492.82857430064\n",
      "RMSE: 177966.5928594334\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"Cleared Data Performance:\")\n",
    "print(f\"MAE: {mae_cleared}\")\n",
    "print(f\"RMSE: {rmse_cleared}\\n\")\n",
    "\n",
    "print(\"Uncleared Data Performance:\")\n",
    "print(f\"MAE: {mae_uncleared}\")\n",
    "print(f\"RMSE: {rmse_uncleared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Regression Results for Cleared Data\n",
    "\n",
    "The Ordinary Least Squares (OLS) regression model was applied to predict the `normalized_price` based on the provided features. The key results from the model summary are as follows:\n",
    "\n",
    "- R-squared: 0.971\n",
    "  - The R-squared value measures the proportion of the variance in the dependent variable (`normalized_price`) that is explained by the independent variables in the model. In this case, the R-squared value of 0.971 indicates that approximately 97.1% of the variance in the `normalized_price` can be explained by the selected features.\n",
    "\n",
    "- F-statistic: 4.494e+04\n",
    "  - The F-statistic tests the overall significance of the model. A high F-statistic indicates that at least one of the predictor variables significantly contributes to explaining the variation in the dependent variable.\n",
    "\n",
    "- P-values and Coefficients:\n",
    "  - Each predictor variable's coefficient represents the change in the predicted `normalized_price` for a one-unit change in the predictor variable while keeping other variables constant.\n",
    "  - The P-value associated with each coefficient tests the null hypothesis that the coefficient is equal to zero (i.e., the variable has no impact on the outcome). A low P-value (usually less than 0.05) suggests that the predictor variable is statistically significant in predicting the `normalized_price`.\n",
    "  - For example, the coefficient for the `size` variable is approximately 398.2325. This means that for each additional unit increase in the `size` of a property, the predicted `normalized_price` is expected to increase by around 398.23 units.\n",
    "\n",
    "Overall, the model's R-squared value suggests that the selected features collectively provide a strong fit to the data. The significance of individual predictor variables is determined by their associated P-values. The coefficients provide insights into the direction and magnitude of the relationships between the predictor variables and the predicted `normalized_price`.\n",
    "\n",
    "## Feature Importance and Uniqueness Analysis\n",
    "\n",
    "Let's delve into the importance and uniqueness of each feature based on their corresponding coefficients, p-values, and t-values from the OLS Regression Results:\n",
    "\n",
    "1. `building_age`\n",
    "   - Coefficient: 115.5670\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `building_age` has a coefficient of 115.5670. This indicates that for each year increase in the building's age, the predicted `normalized_price` is expected to increase by approximately 115.57 units. The low p-value (< 0.001) suggests that this feature is statistically significant and has a strong impact on predicting housing prices.\n",
    "\n",
    "2. `total_floor_count`\n",
    "   - Coefficient: -60.8528\n",
    "   - P-value: 0.198\n",
    "   - Interpretation: The feature `total_floor_count` has a coefficient of -60.8528. This suggests that an increase in the total floor count is associated with a decrease in the predicted `normalized_price` by approximately 60.85 units. However, the relatively higher p-value (0.198) implies that this relationship might not be statistically significant.\n",
    "\n",
    "3. `floor_no`\n",
    "   - Coefficient: 11.5196\n",
    "   - P-value: 0.850\n",
    "   - Interpretation: The feature `floor_no` has a coefficient of 11.5196. This indicates that the specific floor number might have a negligible impact on the predicted `normalized_price`, as the coefficient is relatively low. Additionally, the high p-value (0.850) suggests that this feature is likely not statistically significant.\n",
    "\n",
    "4. `size`\n",
    "   - Coefficient: 398.1804\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `size` has a coefficient of 398.1804. This suggests that for each additional unit increase in the size of the property, the predicted `normalized_price` is expected to increase by around 398.18 units. The low p-value (< 0.001) signifies the strong statistical significance of this feature.\n",
    "\n",
    "5. `District`\n",
    "   - Coefficient: 367.8988\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `District` has a coefficient of 367.8988. This indicates that properties located in different districts have varying impacts on the predicted `normalized_price`. The low p-value (< 0.001) suggests that this categorical feature significantly contributes to predicting housing prices.\n",
    "\n",
    "6. `Neighboorhood`\n",
    "   - Coefficient: 4.9669\n",
    "   - P-value: 0.001\n",
    "   - Interpretation: The feature `Neighboorhood` has a coefficient of 4.9669. This implies that different neighborhoods can have a slight impact on the predicted `normalized_price`. The low p-value (0.001) suggests that this feature is statistically significant.\n",
    "\n",
    "7. `mortgage`\n",
    "   - Coefficient: 0.0121\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `mortgage` has a coefficient of 0.0121. This indicates that for each unit increase in the mortgage value, the predicted `normalized_price` is expected to increase by approximately 0.0121 units. The low p-value (< 0.001) signifies the statistical significance of this feature.\n",
    "\n",
    "8. `ninetyreg`\n",
    "   - Coefficient: -2444.5204\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `ninetyreg` has a coefficient of -2444.5204. This suggests that properties subject to the 90% regulation have a significant impact on predicting housing prices. The low p-value (< 0.001) indicates strong statistical significance.\n",
    "\n",
    "9. `room_count_num`\n",
    "   - Coefficient: -1.062e+04\n",
    "   - P-value: < 0.001\n",
    "   - Interpretation: The feature `room_count_num` has a coefficient of -1.062e+04. This implies that for each increase in the number of rooms, the predicted `normalized_price` is expected to decrease by a considerable amount. The low p-value (< 0.001) reflects the statistical significance of this relationship.\n",
    "\n",
    "10. `avg_room_size`\n",
    "    - Coefficient: 9405.4157\n",
    "    - P-value: < 0.001\n",
    "    - Interpretation: The feature `avg_room_size` has a coefficient of 9405.4157. This indicates that for each increase in the average room size, the predicted `normalized_price` is expected to increase by around 9405 units. The low p-value (< 0.001) signifies the strong statistical significance of this feature.\n",
    "\n",
    "In summary, several features such as `building_age`, `size`, `District`, `mortgage`, `ninetyreg`, `room_count_num`, and `avg_room_size` exhibit strong statistical significance and contribute significantly to predicting housing prices. The impact of other features, such as `total_floor_count` and `floor_no`, might be less pronounced based on their relatively higher p-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       normalized_price   R-squared:                       0.971\n",
      "Model:                            OLS   Adj. R-squared:                  0.971\n",
      "Method:                 Least Squares   F-statistic:                 4.494e+04\n",
      "Date:                Sat, 19 Aug 2023   Prob (F-statistic):               0.00\n",
      "Time:                        16:05:53   Log-Likelihood:            -1.9520e+05\n",
      "No. Observations:               17178   AIC:                         3.904e+05\n",
      "Df Residuals:                   17164   BIC:                         3.905e+05\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================\n",
      "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                    -2.931e+05   3904.575    -75.057      0.000   -3.01e+05   -2.85e+05\n",
      "building_age               115.5670     17.272      6.691      0.000      81.712     149.422\n",
      "total_floor_count          -60.8528     47.240     -1.288      0.198    -153.447      31.742\n",
      "floor_no                    11.5196     61.013      0.189      0.850    -108.071     131.111\n",
      "size                       398.1804     31.788     12.526      0.000     335.872     460.489\n",
      "District                   367.8988     17.870     20.588      0.000     332.872     402.926\n",
      "Neighboorhood                4.9669      1.458      3.406      0.001       2.108       7.826\n",
      "mortgage                     0.0121      0.001     10.312      0.000       0.010       0.014\n",
      "ninetyreg                -2444.5204    505.554     -4.835      0.000   -3435.458   -1453.583\n",
      "room_count_num           -1.062e+04    983.812    -10.791      0.000   -1.25e+04   -8688.187\n",
      "avg_room_size             9405.4157    122.630     76.697      0.000    9165.048    9645.783\n",
      "value_per_room_by_m2      1028.8489   2732.479      0.377      0.707   -4327.090    6384.787\n",
      "diff_from_district_avg   -1029.1230   2732.480     -0.377      0.706   -6385.062    4326.816\n",
      "corrected_price_per_room  -999.3374   2732.480     -0.366      0.715   -6355.277    4356.602\n",
      "==============================================================================\n",
      "Omnibus:                     4854.667   Durbin-Watson:                   1.944\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           109325.250\n",
      "Skew:                           0.828   Prob(JB):                         0.00\n",
      "Kurtosis:                      15.248   Cond. No.                     7.90e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.9e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Add a constant term to the predictor variables matrix\n",
    "X_cleared = sm.add_constant(X_cleared)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y_cleared, X_cleared).fit()\n",
    "\n",
    "# Print the summary of the OLS regression results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Regression Analysis for Uncleared Data\n",
    "\n",
    "### R-squared and F-statistic\n",
    "\n",
    "The R-squared value is an indicator of how well the model fits the data. In this OLS regression, the R-squared value is 0.505. This indicates that approximately 50.5% of the variability in the target variable (`value`) can be explained by the predictor variables included in the model.\n",
    "\n",
    "The F-statistic is used to assess the overall significance of the model. In this case, the F-statistic is 4374. The associated probability (Prob (F-statistic)) is extremely close to 0, indicating that the overall model is statistically significant. This suggests that at least one of the predictor variables has a significant impact on the target variable.\n",
    "\n",
    "### Feature Coefficients, T-values, and P-values\n",
    "\n",
    "Let's delve into the coefficients, t-values, and p-values of the individual features in the OLS regression model:\n",
    "\n",
    "1. `const` (Constant Term):\n",
    "   - Coefficient: 3.042e+04\n",
    "   - T-value: 4.981\n",
    "   - P-value: 0.000\n",
    "   - Interpretation: The constant term represents the baseline value when all other predictor variables are zero. The coefficient of 3.042e+04 indicates the estimated value of the target variable when all other predictor variables are zero. The low p-value (0.000) indicates that the constant term is statistically significant.\n",
    "\n",
    "2. `building_age`:\n",
    "   - Coefficient: 437.6815\n",
    "   - T-value: 3.161\n",
    "   - P-value: 0.002\n",
    "   - Interpretation: The coefficient of 437.6815 implies that for each year increase in `building_age`, the predicted `value` is expected to increase by approximately 437.68 units. The relatively low p-value (0.002) indicates that this feature is statistically significant.\n",
    "\n",
    "3. `total_floor_count`:\n",
    "   - Coefficient: 6748.2805\n",
    "   - T-value: 22.991\n",
    "   - P-value: 0.000\n",
    "   - Interpretation: The coefficient of 6748.2805 suggests that an increase in `total_floor_count` is associated with an increase in the predicted `value` by around 6748.28 units. The very low p-value (0.000) indicates strong statistical significance.\n",
    "\n",
    "4. `size`:\n",
    "   - Coefficient: 1018.8500\n",
    "   - T-value: 23.287\n",
    "   - P-value: 0.000\n",
    "   - Interpretation: The coefficient of 1018.8500 implies that for each additional unit increase in `size`, the predicted `value` is expected to increase by approximately 1018.85 units. The very low p-value (0.000) reflects the statistical significance of this feature.\n",
    "\n",
    "5. `mortgage`:\n",
    "   - Coefficient: 0.7586\n",
    "   - T-value: 122.362\n",
    "   - P-value: 0.000\n",
    "   - Interpretation: The coefficient of 0.7586 indicates that for each unit increase in `mortgage`, the predicted `value` is expected to increase by around 0.7586 units. The very low p-value (0.000) signifies the strong statistical significance of this feature.\n",
    "\n",
    "In summary, the OLS regression results provide insights into the relationships between the predictor variables and the target variable. The features `building_age`, `total_floor_count`, `size`, and `mortgage` exhibit significant impacts on predicting the housing prices (`value`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  value   R-squared:                       0.505\n",
      "Model:                            OLS   Adj. R-squared:                  0.505\n",
      "Method:                 Least Squares   F-statistic:                     4374.\n",
      "Date:                Sat, 19 Aug 2023   Prob (F-statistic):               0.00\n",
      "Time:                        16:12:20   Log-Likelihood:            -2.3157e+05\n",
      "No. Observations:               17178   AIC:                         4.632e+05\n",
      "Df Residuals:                   17173   BIC:                         4.632e+05\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              3.042e+04   6107.288      4.981      0.000    1.85e+04    4.24e+04\n",
      "building_age        437.6815    138.465      3.161      0.002     166.277     709.086\n",
      "total_floor_count  6748.2805    293.524     22.991      0.000    6172.943    7323.618\n",
      "size               1018.8500     43.752     23.287      0.000     933.093    1104.608\n",
      "mortgage              0.7586      0.006    122.362      0.000       0.746       0.771\n",
      "==============================================================================\n",
      "Omnibus:                    10458.556   Durbin-Watson:                   1.955\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           137585.786\n",
      "Skew:                           2.727   Prob(JB):                         0.00\n",
      "Kurtosis:                      15.747   Cond. No.                     1.23e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.23e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant term to the predictor variables matrix\n",
    "X_uncleared = sm.add_constant(X_uncleared)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y_uncleared, X_uncleared).fit()\n",
    "\n",
    "# Print the summary of the OLS regression results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### The Tale of Cleared Data\n",
    "The model trained on the refined, cleared data unfolds as a harmonious composition. With an impressive R-squared value of 0.971, the model encapsulates nearly 97.1% of the variance in normalized housing prices. The F-statistic, a testament to the model's overall significance, emerges at a staggering 4.494e+04. This ensemble of metrics illustrates the adeptness of cleared data in articulating the variance within our target variable.\n",
    "\n",
    "The individual contributors to this symphony, our predictor variables, step onto the stage with significance. Building age, size, district, neighborhood, mortgage, ninetyreg, room count, and average room size join the ensemble, each playing a role in shaping the orchestration of predictions. As we examine the t-values and p-values, we observe a compelling narrative. Building age, size, district, neighborhood, mortgage, ninetyreg, and average room size stand as stalwart features, their p-values painting a picture of statistical significance.\n",
    "\n",
    "### The Story of Uncleared Data\n",
    "On the other hand, the uncleared data paints a different narrative. With an R-squared value of 0.505, this dataset reveals a distinct reality‚Äîapproximately 50.5% of the variability in housing prices is captured. The accompanying F-statistic of 4374 underlines the collective robustness of the model. However, the relatively lower R-squared value can be attributed to the necessary omission of certain features to facilitate the processing of uncleared data. This subtly implies that the dropped features hold a meaningful role in uncovering the genuine prices of the apartments.\n",
    "\n",
    "As we dissect the roles of the individual features, building age, total floor count, size, and mortgage take the spotlight. Each comes forward with their coefficients and significance. The t-values and p-values unveil a mixed landscape‚Äîa landscape where building age, total floor count, size, and mortgage wield statistical significance.   \n",
    "\n",
    "As a result, we have found how much data clearance and feature engineering contribute to the success of data analysis and processing, once again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
